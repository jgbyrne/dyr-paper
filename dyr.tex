\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{cite}

\usepackage[pdftex]{graphicx}
\graphicspath{./figures/}

\usepackage{amsthm, amsmath, amsfonts}
\interdisplaylinepenalty=2500

\usepackage{algorithmic}
\usepackage{array}
\usepackage{url}

\hyphenation{}

\newcommand{\ts}{\textsuperscript}

\begin{document}
\title{Dyr, a Program for Bayesian Inference of Language Phylogenies}

\author{Student Name: J.G. Byrne\\Supervisor Name: Professsor M.J.R. Bordewich\\
Submitted as part of the degree of MEng Computer Science to the\\
Board of Examiners in the Department of Computer Sciences, Durham University
}


\markboth{DURHAM UNIVERSITY, DEPARTMENT OF COMPUTER SCIENCE}%
{Shell \MakeLowercase{\textit{et al.}}}

\IEEEtitleabstractindextext{%
\begin{abstract}
    Bayesian statistics provide a powerful framework for inferring the evolutionary history of language families from lexical data, a problem which is made computationally tractable by Markov Chain Monte Carlo (MCMC) algorithms. We present Dyr, a simple yet capable new system for Bayesian inference, and use it to replicate state-of-the-art results in the field of Indo-European linguistics. 
\end{abstract}

\begin{IEEEkeywords}
Bayesian Inference, Markov Processes, Linguistics, Indo-European
\end{IEEEkeywords}}

\maketitle
\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle

\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

\IEEEPARstart{L}{anguages} evolve over time. Some of these changes are phonetic or grammatical, but many are lexical (changes in the vocabulary). Words are forgotten and replaced, sometimes by other words that have changed meaning, and sometimes by borrowings from other languages.\footnote{For example, the Old English word \textit{dēor} was displaced by the French 'animal', and survives only in the narrower sense 'deer'. However, the equivalent word in Danish, \textit{dyr}, whence this project name, retains the older meaning. Both words derive from Proto-Germanic \textit{*deuzą}, ultimately from Proto-Indo-European \textit{*d\ts{h}wes}, meaning 'breath'.} Occasionally, some speakers of a language will come to speak so differently to the others that the groups can no longer understand each other, and the language splits in two.

Scholars have noticed similarities between languages for millenia. In the 18\ts{th} century, the field of comparative linguistics was born, which seeks to systematically reconstruct the historical relationships between languages. The early comparative linguists observed that while languages can evolve, sunder, and go extinct, they rarely merge, and are never created anew. Therefore, from the 19\ts{th} century onwards it became \textit{de rigeur} to describe language descent with evolutionary trees, or `phylogenies', and though not without criticism, this model remains predominant in linguistics to this day.

Historically, constructing phylogenies has been a job for humans, being based on the judgement of expert scholars with deep knowledge of languages ancient and modern. Although fruitful, this approach is naturally susceptible to human biases and oversights. It also provides no way to determine the age of unattested ancestor languages other than the (admittedly finely-honed) intuition of learned intellectuals.

The problem of ancestral dating is of particular relevance to Indo-European, the largest and most studied language family in the world. Encompassing nearly all of the languages of Europe and a great many in Western Asia and India, the challenge of reconstructing the history of this vast grouping has captivated comparative linguists since the advent of the discipline. And yet, one central question is yet to be conclusively answered: where and when was Proto-Indo-European (PIE; the ancestor of all Indo-European languages) originally spoken? Two main theories abound. The first, known as the `Kurgan' hypothesis, postulates an \textit{Urheimat} (original homeland) in the Pontic-Caspian steppe north of the Black Sea\cite{gimbutas1974gods}.\footnote{A \textit{kurgan} is a tumulus or burial mound. This theory is associates the spread of Indo-European with warlike tumulus-building charioteers.} Conversely, the second proposes an origin in Anatolia\cite{renfrew2001anatolian}.\footnote{This rather more sedate theory suggests a gradual proliferation of Indo-European in tandem with the spread of agriculture} The crucial difference between the two theories is that while the former ascribes a time depth of 6000 years to PIE, the latter hinges on it being considerably more ancient, at approximately 8500 years old. A reliable estimate for the age of PIE - that is, the root age of the Indo-European language tree - could therefore be potent evidence for one theory over the other.

Since the millenium, researchers have adopted a new method to analyse language families in general and Indo-European in particular -- Bayesian inference. By re-appropriating software designed to analyse genetic data and construct biological evolutionary trees, they have successfully inferred plausible dated phylogenies from large vocabulary databases. Such research feels tantalisingly close to an objective solution to linguistic enigmas such as that of Indo-European provenance.

However, in reality, the human factor still plays a role. Bayesian inference requires the specification of prior distributions, the choice of which can radically affect the results of the computation. For research to be rigorous and reliable, priors should be chosen carefully and properly justified.

Yet much study in the relatively small field of `Bayesian Phylolinguistics' falls short of this mark. Priors are often chosen seemingly out of habit or convention. Such laxness poses particular danger when these conventions have their origins in bioinformatics; a model which is sensible in the context of biological evolution may not be so logical when applied to languages.

We suggest that one reason for this tendency is the continued reliance on large, featureful bioinformatics software packages like BEAST2 and MrBayes. Any dedicated support for linguistics in these programs tends to be something of an afterthought, and their intimidating size makes them hard to understand in their totality. Therefore, we submit that to step out of the shadow of the older, larger discipline, it is desirable for Bayesian phylolinguistics to have its own dedicated software package.

We thusly present \textit{Dyr}, a new program for Bayesian inference of linguistic phylogenies. Small enough that its source code may be read and understood in a day, and offering only the features required for linguistic analysis, \textit{Dyr} is nonetheless built to be flexible and extensible. Capable of replicating state-of-the-art results in Bayesian Phylolinguistics, our hope and intention is that \textit{Dyr} can serve as a worthy platform for trialling novel methods with sound linguistic justification.

\section{Related Work}

The first serious attempt to use statistical methods to investigate language phylogenies was made by Morris Swadesh in the 1950s. His `glottochronology' model -- which assumed a constant global rate of change -- had limited practical utility, but his ideas were foundational to more sophisticated subsequent attempts.\cite{swadesh1955towards} In particular, the concept of the Swadesh list, a collation of basic and universal word-semantics underlies the lexical trait analysis used in quantitative historical linguistics today, including in this report.
\subsection{Inferring Indo-European}

The first notable paper to use the Markov Chain Monte Carlo (MCMC) technique to infer the posterior distribution of a linguistic phylogeny was Gray and Atkinson's groundbreaking 2003 study. Their analysis of the Indo-European family, which employed the forerunner of the modern IELEX dataset, yielded robust support for the Anatolian hypothesis.\cite{gray2003language} This research was reinforced by the later results of Bouckaert et al.\cite{bouckaert2012mapping}\cite{bouckaert2013correction}, who used an updated dataset, and tested an alternative trait model devised by Nicholls and Gray\cite{nicholls2008dated} requiring that cognate traits arise just once in a phylogeny, again finding strongest support for an Anatolian origin.

This trend was bucked by Chang et al., who built upon the work of Bouckaert et al. with the key addition of novel prior constraints on tree topology.\cite{chang2015ancestry} In particular, they mandated that certain languages be effectively ancestral to others in the tree; for example, Old English is necessarily positioned as a direct ancestor to English. Their intention was to correct for a phenomenon that they dubbed `jogging'. Without ancestry constraints, an ancestral interior node will be postulated that is older than both a modern language and its more ancient ancestor. Consequently the `traversal distance' between the two languages will be substantially longer than should actually be the case. This causes the Bayesian process to infer generally lower evolutionary rates and thus an older root age for the tree. With these constraints imposed, and with some fixes applied to the Bouckaert et al. dataset, Chang et al. found the strongest support for the Steppe hypothesis.

Since the publication of the Chang et al. study, futher research has questioned the necessity of such constraints. Rama suggested that similar results to those obtained by Chang et al. could be obtained without ancestry constraints by usage of a Uniform tree prior.\cite{rama2018three} Rama astutely observed that the population size parameter of the Coalescent prior does not have an obvious interpretation in the context of language evolution, but neglected to apply the same standard of justifiability to the Uniform prior. This is in spite of the clear pitfalls of the Uniform prior; namely, that it implies a language family evolves as a collective whole, rather than evolution being a process happening largely independantly in each individual language. However, his work does serve to highlight the potential for further investigation into the bread-and-butter of phylogenetic inference; prior distributions and substitution models. In particular we believe Rama's work demonstrates a neccessity for research into tree prior models that are both empirically successful at replicating validated results and also have sound linguistic justification. 

\subsection{Extant Software}

Bayesian phylogenetic inference is dominated by two large software packages, MrBayes\cite{ronquist2012mrbayes} and BEAST 2\cite{bouckaert2014beast}. The former is written in C while the latter uses Java -- languages which lack features that many modern programmers value, such as Algebraic Data Types. Both libaries are extremely featureful, but many features are inappropriate in a linguistics context or are simply inapplicable. The size of these software packages makes them hard to study and modify; for example, MrBayes has nearly 100,000 lines of C code, of which 20,000 is the MCMC implementation alone.

Both BEAST 2 and MrBayes were originally written with Bioinformaticians as the target users; to my knowledge \textit{Dyr} is the first Bayesian inference package designed from the ground-up with phylolinguistics as the primary use-case. \textit{Dyr} follows the lead of both these software packages by using the BEAGLE3 library for hardware-accelerated computation of likelihoods.\cite{ayres2019beagle}

\section{Methods}

The aim of phylogenetic inference is to learn which phylogenies (dated trees) are most likely given a particular evolutionary model and a set of known data. This is known as the `posterior likelihood'. In a linguistics context, the known data is typically `lexical trait data' -- that is, information about the vocabulary of the languages in question. The evolutionary model, which in Bayesian terms provides our `prior likelihood', encapsulates our beliefs about how likely languages are to diverge and how rapidly their vocabulary changes. To allow our inferred phylogeny to best fit the signal present in the data, we also infer some parameters of our evolutionary model, though these too are subject to their own prior likelihood distributions.

The space of possible paramaterisations is very large and it is not typically feasible to derive a closed-form expression for the posterior likelihood distribution. However, Bayes' theorem allows us to assess the posterior likelihood of any specific choice of paramaterisation given the data. We therefore use a stochastic process called Markov Chain Monte Carlo (MCMC) to iteratively step-through the space of possible paramaterisations, with a preference for augmentations to the paramaterisation that improve the posterior likelihood. It is provable that the long-run outcome of this process will be to simulate the desired posterior distribution.

\subsection{Lexical Trait Data}

The primary evidence used to infer linguistic phylogenies is lexical trait data. In principle, many different linguistic features could be appropriated to inform Bayesian inference, but for both principled and pragmatic reasons the vast majority of analyses use lexical data. In particular, the class of traits used in our datasets is what Chang et al. termed \textsc{Root Meaning} traits. These traits can be described as tuples in the form (\textit{ root },\;\texttt{semantic}\;). A \textsc{Root Meaning} trait is binary; either present or absent for a given language. A given trait is present in a language if that language has a common word for the \texttt{semantic} that derives from the \textit{root}. For example, the Irish word for a \texttt{fish} is `iasc', which like the English `fish', comes from the Indo-European root \textit{*peysk-}. So the trait (\textit{ *peysk- },\;\texttt{fish}\;) is present in both Irish and English. However the Greek word for \texttt{fish} is `ikhthus', which is believed to derive from the root \textit{*d\ts{h}g\ts{h}u-}. Therefore the trait is absent in Greek.

From a great many such traits is constructed the IELEX database, the gold-standard source of lexical data for Indo-European, upon which we shall found our analyses. In particular, we use the subsets of IELEX constructed by Chang et al., termed \textsc{Narrow}, \textsc{Medium}, and \textsc{Broad}, containing 52, 82, and 94 languages respectively. In our inferences, these languages will correspond to leaves in our phylogenies.

\subsection{Defining the Posterior Distribution}

Bayes' theorem is stated as follows:
\begin{equation}
    Pr(A\;|\;B) = \frac{Pr(B\;|\;A) \cdot Pr(A)}{Pr(B)}
\end{equation}

In the context of phylogenetic inference, we seek to infer the probability distribution of possible parameterisations given the observed data. Therefore $Pr(B)$ corresponds to $Pr(x)$, the probability of the trait data, which is by definition equal to $1$ since it has been observed. Meanwhile, $Pr(A)$ corresponds to $Pr(\Gamma)$, the prior likelihood of a given parameterisation. We define $\Gamma$ more thoroughly below:
\begin{align*}
    \Gamma &= (\psi, \omega, \lambda)\\
    \psi   &= \text{parameters describing a dated tree}\\
    \omega &= \text{parameters of the prior model for dated trees}\\
    \lambda&= \text{parameters of the prior model for trait evolution}
\end{align*}
We thus derive equation \eqref{eqn:posterior}. The posterior likelihood is proportional to the likelihood of the data given the paramaterisation multiplied by the prior likelihood of the paramaterisation. This is a proportionality because we do not require that our prior distributions sum to $1$.
\begin{equation}\label{eqn:posterior}
    Pr(\psi, \omega, \lambda\;|\; x) \propto Pr(x\;|\;\psi, \omega, \lambda) \cdot f(\psi\;|\;\omega) \cdot f(\omega) \cdot f(\lambda)
\end{equation}

We define $\psi = (\tau, \delta)$, where $\tau$ is the topology of the tree and $\delta$ is the branch lengths. We consider the node or nodes with the longest path from the root to be at $t = 0$ and therefore in the present day, while all other nodes are understood to be at some $t > 0$, corresponding to their distance from the present in years. Naturally, any interior node is required to be older than its children.

\subsection{Calculating Likelihood}

Although Bayes' theorem absolves us of the need to directly calculate the posterior, we must still calculate the likelihood of the data given the paramaterisation. Conceptually speaking, this is the likelihood across all possible trait assignments (that is, a binary array of absences and presences) across all possible nodes of the tree of the observed trait data being evolved at the tips, under a given process of trait evolution we call the substitution model.

\subsubsection{Felsenstein's Algorithm}

At first glance, this calculation sounds intractable. However, it can in fact be computed, with a divide-and-conquer method called Felsenstein's algorithm. The procedure is fairly inutitive: for each trait, we first define the trait's likelihood at the tips, based on the observed data. Then we work our way up the tree, deriving the `partial' likelihoods at each interior node based on its two children, until we reach the root, where we can sum up a final likelihood for the trait across all assignments. The overall likelihood of the tree given the data is therefore simply the product across all traits.\cite{felsenstein2004inferring}

Equation \eqref{eqn:felsenstein} below gives a formal definition of Felsenstein's algorithm. We denote the likelihood of a node $x$ having state $t$ for the trait $i$ as $L_x^{i}(t)$, and it is defined separately for leaf nodes $\ell$ and for interior nodes $a$. It's worth remembering that for our purposes the set of states is simply $t \in \{0, 1\}$, but the algorithm is best understood in its full generality. When a leaf node $\ell$ has recorded data for trait $i$, the value $x_\ell^{i}\left[\;t\;\right]$ is $1.0$ for the observed state $t$ and $0.0$ otherwise. When instead, the data is missing, the likelihood is split evenly between all values of $t$. For an interior node $a$, the likelihood of having state $t$ for the trait $i$ is calculated based on the product of the sum likelihood across all possible ways $t$ can evolve along the branch to the left child $a$ and the equivalent calculation along the branch to the right child $b$. The evolution probability is the core calculation performed by the substitution model and is predicated on the parameters $\lambda$. Note how the likelihood is thus recursively `rolled-up' towards $r$, the root node of the tree.

\begin{equation}\label{eqn:felsenstein}
    \setlength{\jot}{12px}
    \begin{split}
        L_\ell^{i}(t) &= x_\ell^{i}\left[\;t\;\right] \\
        L_a^{i}(t) &= \left(\sum_uPr^i_{a,b}\left(u\,\vert\,t\right)L_b^{i}\left(u\right)\right)\left(\sum_vPr^i_{a,c}\left(v\,\vert\,t\right)L_c^{i}\left(v\right)\right) \\
        \mathcal{L}^{i} &= \sum_u \;\pi_u \cdot L_r^{i}\left(u\right) \\
        \mathcal{L} &= \prod_i \;\mathcal{L}^i
    \end{split}
\end{equation}

We denote the root likelihood across all states of trait $i$ as $\mathcal{L}^{i}$. The calculation required is a weighted average, with the weights being the stationary frequencies $\pi_u$, which will be discussed shortly along with the other parameters in $\lambda$.

Calculating the final root likelihood is then a simple matter of taking the product of all of the per-trait root likelihoods. This value, $\mathcal{L}$, is the value of the term $Pr(x\;|\;\psi, \omega, \lambda)$ in equation \eqref{eqn:posterior}.

\subsubsection{Substitution Model and Trait Evolution}

We now seek to explain how the substitution model is used to calculate the probability of a state $t$ evolving to a state $u$ along a given edge of a tree. Let us state upfront the parameters of the model:

\begin{align*}
    \lambda &= (\mu,\;\pi,\;\alpha,\;\beta,\;\phi)\\
    \mu &= \text{Substitution base rate}\\
    \pi &= \text{Stationary frequencies: } \pi_0, \pi_1 \in \left(0, 1\right) : \pi_0 + \pi_1 = 1.0\\
    \alpha &= \text{Shape for Among Site Rate Variation (ASRV)}\\
    \beta &= \text{Shape for Among Branch Rate Variation (ABRV)}\\
    \phi &= \text{ABRV Rate Assignments}
\end{align*}

The core of the substitution model is the rate matrix, which describes the rates at which states mutate into each other. These matrices can be quite complex, but we opt for the simplest choice, the Generalised Time Reversible (GTR) model.\footnote{Chang et al. calls this the Restriction Site Character (RSC) model; the two are equivalent in the case of binary traits.} We have already seen how the stationary frequencies $\pi_0$ and $\pi_1$ influence the likelihood calculation, but they are also at the heart of the binary GTR rate matrix. 
\begin{equation}
\textbf{Q} = \frac{1}{2\pi_0\pi_1} \begin{bmatrix}
-\pi_1 & \pi_1\\
\pi_0 & -\pi_0
\end{bmatrix} \text{ \ given that \ } \pi_0 + \pi_1 = 1.0
\end{equation}

As the length of time over which a trait evolves increases, the probability of it being in a state $u$ asymptopically approaches $\pi_u$, and the state it was initially in becomes irrelevant. To be more precise, for every branch $(a, b)$ in the tree $\psi$, for every trait $i$, the transition probabilities $Pr^i_{a,b}\left(u\,\vert\,t\right)$ are defined by the transition matrix $\textbf{P}^i_{a,b}$, where:

\begin{equation}
\textbf{P}^i_{a,b} = \exp\left(\textbf{Q} \cdot \delta_{a,b} \cdot \eta^i_{a,b}\right) \text{  \ where  \  } \eta^i_{a,b} = \mu \cdot \gamma_i \cdot \rho_{a,b}
\end{equation}

As previously stated, the value $\delta_{a,b}$ is the length (in years) of the branch $(a, b)$. The value $\eta^i_{a,b}$ is the rate for the trait $i$ on the branch $(a, b)$, calculated as the product of three rate parameters. The first rate,  $\mu$, is the base rate -- a global parameter that cancels out the units of the branch lengths and controls the overall rate of evolution. The second, $\gamma_i$, is the site rate,\footnote{The term `site' is a relic from bioinformatics, where traits typically correspond to specific sites on the genome} which is specific to this trait $i$. It is drawn from a Gamma distribution $\Gamma(\alpha, \frac{1}{\alpha})$. The third rate, $\rho_{a,b}$, is the branch rate, which is specific to this branch. It is drawn from a log-normal distribution $\log \mathcal{N}(-\frac{\beta^2}{2}, \beta^2)$. Both of these distributions have a mean of $1$, so regardless of their shapes the overall average rate is equal to $\mu$. The choices of distributions are partly informed by implementation pragamatics and partly by the flexibility in shape that can be attained by modfications to $\alpha$ and $\beta$. \footnote{This paragraph abstracts quite significantly over the specifics of how these rates are actually chosen, which shall be discussed later.}

At this point the reader may be questioning the necessity of allowing rates to vary both across sites and and across branches. However there is a strong justification for both laxities. Variation in site rate is necessary because words vary greatly in their volatility. Certain common words, in particular numerals, change incredibly rarely. Others are considerably more susceptible to replacement. Equally, branch rates must be variable because languages evolve at dramatically different rates. A failure to account for this fact was the downfall of much early research into quantitiative historical linguistics. The classic (though by no means sole) exemplar is the case of the Nordic languages; while Norwegians find Old Norse virtually incomprehensible, Icelanders can read it as easily as an Englishman can read Jane Austen. The reason for this is simply that the rate of change of Icelandic has been remarkably slow, while that of Norwegian has been reasonably fast.

\subsection{Prior Distributions}

\subsubsection{Coalescent Theory}

\subsection{Markov Chain Monte Carlo}

\section{Implementation}

\subsection{Program Structure}

\subsection{Elegance and Extensibility}

\section{Results}

\subsection{Comparison with Other Research}

\subsection{Performance Analysis}

\section{Evaluation and Future Steps}

\section{Conclusion}

\newpage

\bibliographystyle{ieeetran}
\bibliography{dyr}

\end{document}
