\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{cite}

\usepackage[pdftex]{graphicx}
\graphicspath{./figures/}

\usepackage{amsthm, amsmath, amsfonts}
\interdisplaylinepenalty=2500

\usepackage{algorithmic}
\usepackage{array}
\usepackage{url}

\hyphenation{}

\newcommand{\ts}{\textsuperscript}

\begin{document}
\title{Dyr, a Program for Bayesian Inference of Language Phylogenies}

\author{Student Name: J.G. Byrne\\Supervisor Name: Professsor M.J.R. Bordewich\\
Submitted as part of the degree of MEng Computer Science to the\\
Board of Examiners in the Department of Computer Sciences, Durham University
}


\markboth{DURHAM UNIVERSITY, DEPARTMENT OF COMPUTER SCIENCE}%
{Shell \MakeLowercase{\textit{et al.}}}

\IEEEtitleabstractindextext{%
\begin{abstract}
    Bayesian statistics provide a powerful framework for inferring the evolutionary history of language families from lexical data, a problem which is made computationally tractable by Markov Chain Monte Carlo (MCMC) algorithms. We present Dyr, a simple yet capable new system for Bayesian inference, and use it to replicate state-of-the-art results in the field of Indo-European linguistics. 
\end{abstract}

\begin{IEEEkeywords}
Bayesian Inference, Markov Processes, Linguistics, Indo-European
\end{IEEEkeywords}}

\maketitle
\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle

\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

\IEEEPARstart{L}{anguages} evolve over time. Some of these changes are phonetic or grammatical, but many are lexical (changes in the vocabulary). Words are forgotten and replaced, sometimes by other words that have changed meaning, and sometimes by borrowings from other languages.\footnote{For example, the Old English word \textit{dēor} was displaced by the French 'animal', and survives only in the narrower sense 'deer'. However, the equivalent word in Danish, \textit{dyr}, whence this project name, retains the older meaning. Both words derive from Proto-Germanic \textit{*deuzą}, ultimately from Proto-Indo-European \textit{*d\ts{h}wes}, meaning 'breath'.} Occasionally, some speakers of a language will come to speak so differently to the others that the groups can no longer understand each other, and the language splits in two.

Scholars have noticed similarities between languages for millenia. In the 18\ts{th} century, the field of comparative linguistics was born, which seeks to systematically reconstruct the historical relationships between languages. The early comparative linguists observed that while languages can evolve, sunder, and go extinct, they rarely merge, and are never created anew. Therefore, from the 19\ts{th} century onwards it became \textit{de rigeur} to describe language descent with evolutionary trees, or `phylogenies', and though not without criticism, this model remains predominant in linguistics to this day.

Historically, constructing phylogenies has been a job for humans, being based on the judgement of expert scholars with deep knowledge of languages ancient and modern. Although fruitful, this approach is naturally susceptible to human biases and oversights. It also provides no way to determine the age of unattested ancestor languages other than the (admittedly finely-honed) intuition of learned intellectuals.

The problem of ancestral dating is of particular relevance to Indo-European, the largest and most studied language family in the world. Encompassing nearly all of the languages of Europe and a great many in Western Asia and India, the challenge of reconstructing the history of this vast grouping has captivated comparative linguists since the advent of the discipline. And yet, one central question is yet to be conclusively answered: where and when was Proto-Indo-European (PIE; the ancestor of all Indo-European languages) originally spoken? Two main theories abound. The first, known as the `Kurgan' hypothesis, postulates an \textit{Urheimat} (original homeland) in the Pontic-Caspian steppe north of the Black Sea\cite{gimbutas1974gods}.\footnote{A \textit{kurgan} is a tumulus or burial mound. This theory is associates the spread of Indo-European with warlike tumulus-building charioteers.} Conversely, the second proposes an origin in Anatolia\cite{renfrew2001anatolian}.\footnote{This rather more sedate theory suggests a gradual proliferation of Indo-European in tandem with the spread of agriculture} The crucial difference between the two theories is that while the former ascribes a time depth of 6000 years to PIE, the latter hinges on it being considerably more ancient, at approximately 8500 years old. A reliable estimate for the age of PIE - that is, the root age of the Indo-European language tree - could therefore be potent evidence for one theory over the other.

Since the millenium, researchers have adopted a new method to analyse language families in general and Indo-European in particular -- Bayesian inference. By re-appropriating software designed to analyse genetic data and construct biological evolutionary trees, they have successfully inferred plausible dated phylogenies from large vocabulary databases. Such research feels tantalisingly close to an objective solution to linguistic enigmas such as that of Indo-European provenance.

However, in reality, the human factor still plays a role. Bayesian inference requires the specification of prior distributions, the choice of which can radically affect the results of the computation. For research to be rigorous and reliable, priors should be chosen carefully and properly justified.

Yet much study in the relatively small field of `Bayesian Phylolinguistics' falls short of this mark. Priors are often chosen seemingly out of habit or convention. Such laxness poses particular danger when these conventions have their origins in bioinformatics; a model which is sensible in the context of biological evolution may not be so logical when applied to languages.

We suggest that one reason for this tendency is the continued reliance on large, featureful bioinformatics software packages like BEAST2 and MrBayes. Any dedicated support for linguistics in these programs tends to be something of an afterthought, and their intimidating size makes them hard to understand in their totality. Therefore, we submit that to step out of the shadow of the older, larger discipline, it is desirable for Bayesian phylolinguistics to have its own dedicated software package.

We thusly present \textit{Dyr}, a new program for Bayesian inference of linguistic phylogenies. Small enough that its source code may be read and understood in a day, and offering only the features required for linguistic analysis, \textit{Dyr} is nonetheless built to be flexible and extensible. Capable of replicating state-of-the-art results in Bayesian Phylolinguistics, our hope and intention is that \textit{Dyr} can serve as a worthy platform for trialling novel methods with sound linguistic justification.

\section{Related Work}

The first serious attempt to use statistical methods to investigate language phylogenies was made by Morris Swadesh in the 1950s. His `glottochronology' model -- which assumed a constant global rate of change -- had limited practical utility, but his ideas were foundational to more sophisticated subsequent attempts.\cite{swadesh1955towards} In particular, the concept of the Swadesh list, a collation of basic and universal word-semantics underlies the lexical trait analysis used in quantitative historical linguistics today, including in this report.
\subsection{Inferring Indo-European}

The first notable paper to use the Markov Chain Monte Carlo (MCMC) technique to infer the posterior distribution of a linguistic phylogeny was Gray and Atkinson's groundbreaking 2003 study. Their analysis of the Indo-European family, which employed the forerunner of the modern IELEX dataset, yielded robust support for the Anatolian hypothesis.\cite{gray2003language} This research was reinforced by the later results of Bouckaert et al.\cite{bouckaert2012mapping}\cite{bouckaert2013correction}, who used an updated dataset, and tested an alternative trait model devised by Nicholls and Gray\cite{nicholls2008dated} requiring that cognate traits arise just once in a phylogeny, again finding strongest support for an Anatolian origin.

This trend was bucked by Chang et al., who built upon the work of Bouckaert et al. with the key addition of novel prior constraints on tree topology.\cite{chang2015ancestry} In particular, they mandated that certain languages be effectively ancestral to others in the tree; for example, Old English is necessarily positioned as a direct ancestor to English. Their intention was to correct for a phenomenon that they dubbed `jogging'. Without ancestry constraints, an ancestral interior node will be postulated that is older than both a modern language and its more ancient ancestor. Consequently the `traversal distance' between the two languages will be substantially longer than should actually be the case. This causes the Bayesian process to infer generally lower evolutionary rates and thus an older root age for the tree. With these constraints imposed, and with some fixes applied to the Bouckaert et al. dataset, Chang et al. found the strongest support for the Steppe hypothesis.

Since the publication of the Chang et al. study, futher research has questioned the necessity of such constraints. Rama suggested that similar results to those obtained by Chang et al. could be obtained without ancestry constraints by usage of a Uniform tree prior.\cite{rama2018three} Rama astutely observed that the population size parameter of the Coalescent prior does not have an obvious interpretation in the context of language evolution, but neglected to apply the same standard of justifiability to the Uniform prior. This is in spite of the clear pitfalls of the Uniform prior; namely, that it implies a language family evolves as a collective whole, rather than evolution being a process happening largely independantly in each individual language. However, his work does serve to highlight the potential for further investigation into the bread-and-butter of phylogenetic inference; prior distributions and substitution models. In particular we believe Rama's work demonstrates a neccessity for research into tree prior models that are both empirically successful at replicating validated results and also have sound linguistic justification. 

\subsection{Extant Software}

Bayesian phylogenetic inference is dominated by two large software packages, MrBayes\cite{ronquist2012mrbayes} and BEAST 2\cite{bouckaert2014beast}. The former is written in C while the latter uses Java -- languages which lack features that many modern programmers value, such as Algebraic Data Types. Both libaries are extremely featureful, but many features are inappropriate in a linguistics context or are simply inapplicable. The size of these software packages makes them hard to study and modify; for example, MrBayes has nearly 100,000 lines of C code, of which 20,000 is the MCMC implementation alone.

Both BEAST 2 and MrBayes were originally written with Bioinformaticians as the target users; to my knowledge \textit{Dyr} is the first Bayesian inference package designed from the ground-up with phylolinguistics as the primary use-case. \textit{Dyr} follows the lead of both these software packages by using the BEAGLE3 library for hardware-accelerated computation of likelihoods.\cite{ayres2019beagle}

\section{Methods}

The aim of phylogenetic inference is to learn which phylogenies (dated trees) are most likely given a particular evolutionary model and a set of known data. This is known as the `posterior likelihood'. In a linguistics context, the known data is typically `lexical trait data' -- that is, information about the vocabulary of the languages in question. The evolutionary model, which in Bayesian terms provides our `prior likelihood', encapsulates our beliefs about how likely languages are to diverge and how rapidly their vocabulary changes. To allow our inferred phylogeny to best fit the signal present in the data, we also infer some parameters of our evolutionary model, though these too are subject to their own prior likelihood distributions.

The space of possible paramaterisations is very large and it is not typically feasible to derive a closed-form expression for the posterior likelihood distribution. However, Bayes' theorem allows us to assess the posterior likelihood of any specific choice of paramaterisation given the data. We therefore use a stochastic process called Markov Chain Monte Carlo (MCMC) to iteratively step-through the space of possible paramaterisations, with a preference for augmentations to the paramaterisation that improve the posterior likelihood. It is provable that the long-run outcome of this process will be to simulate the desired posterior distribution.

\subsection{Lexical Trait Data}

The primary evidence used to infer linguistic phylogenies is lexical trait data. In principle, many different linguistic features could be appropriated to inform Bayesian inference, but for both principled and pragmatic reasons the vast majority of analyses use lexical data. In particular, the class of traits used in our datasets is what Chang et al. termed \textsc{Root Meaning} traits. These traits can be described as tuples in the form (\textit{ root },\;\texttt{semantic}\;). A \textsc{Root Meaning} trait is binary; either present or absent for a given language. A given trait is present in a language if that language has a common word for the \texttt{semantic} that derives from the \textit{root}. For example, the Irish word for a \texttt{fish} is `iasc', which like the English `fish', comes from the Indo-European root \textit{*peysk-}. So the trait (\textit{ *peysk- },\;\texttt{fish}\;) is present in both Irish and English. However the Greek word for \texttt{fish} is `ikhthus', which is believed to derive from the root \textit{*d\ts{h}g\ts{h}u-}. Therefore the trait is absent in Greek.

From a great many such traits is constructed the IELEX database, the gold-standard source of lexical data for Indo-European, upon which we shall found our analyses. In particular, we use the subsets of IELEX constructed by Chang et al., termed \textsc{Narrow}, \textsc{Medium}, and \textsc{Broad}, containing 52, 82, and 94 languages respectively. In our inferences, these languages will correspond to leaves in our phylogenies.

\subsection{Defining the Posterior Distribution}

Bayes' theorem is stated as follows:
\begin{equation}
    Pr(A\;|\;B) = \frac{Pr(B\;|\;A) \cdot Pr(A)}{Pr(B)}
\end{equation}

In the context of phylogenetic inference, we seek to infer the probability distribution of possible parameterisations given the observed data. Therefore $Pr(B)$ corresponds to $Pr(x)$, the probability of the trait data, which is by definition equal to $1$ since it has been observed. Meanwhile, $Pr(A)$ corresponds to $Pr(\Gamma)$, the prior likelihood of a given parameterisation. We define $\Gamma$ more thoroughly below:
\begin{align*}
    \Gamma &= (\psi, \omega, \lambda)\\
    \psi   &= \text{parameters describing a dated tree}\\
    \omega &= \text{parameters of the prior model for dated trees}\\
    \lambda&= \text{parameters of the prior model for trait evolution}
\end{align*}
We thus derive equation \eqref{eqn:posterior}. The posterior likelihood is proportional to the likelihood of the data given the paramaterisation multiplied by the prior likelihood of the paramaterisation. This is a proportionality because we do not require that our prior distributions sum to $1$.
\begin{equation}\label{eqn:posterior}
    Pr(\psi, \omega, \lambda\;|\; x) \propto Pr(x\;|\;\psi, \omega, \lambda) \cdot f(\psi\;|\;\omega) \cdot f(\omega) \cdot f(\lambda)
\end{equation}

We define $\psi = (\tau, \delta)$, where $\tau$ is the topology of the tree and $\delta$ is the branch lengths. We consider the node or nodes with the longest path from the root to be at $t = 0$ and therefore in the present day, while all other nodes are understood to be at some $t > 0$, corresponding to their distance from the present in years. Naturally, any interior node is required to be older than its children.

\subsection{Calculating Likelihood}

Although Bayes' theorem absolves us of the need to directly calculate the posterior, we must still calculate the likelihood of the data given the paramaterisation. Conceptually speaking, this is the likelihood across all possible trait assignments (that is, a binary array of absences and presences) across all possible nodes of the tree of the observed trait data being evolved at the tips, under a given process of trait evolution we call the substitution model.

\subsubsection{Felsenstein's Algorithm}

At first glance, this calculation sounds intractable. However, it can in fact be computed, with a divide-and-conquer method called Felsenstein's algorithm. The procedure is fairly inutitive: for each trait, we first define the trait's likelihood at the tips, based on the observed data. Then we work our way up the tree, deriving the `partial' likelihoods at each interior node based on its two children, until we reach the root, where we can sum up a final likelihood for the trait across all assignments. The overall likelihood of the tree given the data is therefore simply the product across all traits.\cite{felsenstein2004inferring}

Equation \eqref{eqn:felsenstein} below gives a formal definition of Felsenstein's algorithm. We denote the likelihood of a node $x$ having state $t$ for the trait $i$ as $L_x^{i}(t)$, and it is defined separately for leaf nodes $\ell$ and for interior nodes $a$. It's worth remembering that for our purposes the set of states is simply $t \in \{0, 1\}$, but the algorithm is best understood in its full generality. When a leaf node $\ell$ has recorded data for trait $i$, the value $x_\ell^{i}\left[\;t\;\right]$ is $1.0$ for the observed state $t$ and $0.0$ otherwise. When instead, the data is missing, the likelihood is split evenly between all values of $t$. For an interior node $a$, the likelihood of having state $t$ for the trait $i$ is calculated based on the product of the sum likelihood across all possible ways $t$ can evolve along the branch to the left child $a$ and the equivalent calculation along the branch to the right child $b$. The evolution probability is the core calculation performed by the substitution model and is predicated on the parameters $\lambda$. Note how the likelihood is thus recursively `rolled-up' towards $r$, the root node of the tree.

\begin{equation}\label{eqn:felsenstein}
    \setlength{\jot}{12px}
    \begin{split}
        L_\ell^{i}(t) &= x_\ell^{i}\left[\;t\;\right] \\
        L_a^{i}(t) &= \left(\sum_uPr^i_{a,b}\left(u\,\vert\,t\right)L_b^{i}\left(u\right)\right)\left(\sum_vPr^i_{a,c}\left(v\,\vert\,t\right)L_c^{i}\left(v\right)\right) \\
        \mathcal{L}^{i} &= \sum_u \;\pi_u \cdot L_r^{i}\left(u\right) \\
        \mathcal{L} &= \prod_i \;\mathcal{L}^i
    \end{split}
\end{equation}

We denote the root likelihood across all states of trait $i$ as $\mathcal{L}^{i}$. The calculation required is a weighted average, with the weights being the stationary frequencies $\pi_u$, which will be discussed shortly along with the other parameters in $\lambda$.

Calculating the final root likelihood is then a simple matter of taking the product of all of the per-trait root likelihoods. This value, $\mathcal{L}$, is the value of the term $Pr(x\;|\;\psi, \omega, \lambda)$ in equation \eqref{eqn:posterior}.

\subsubsection{Substitution Model and Trait Evolution}

We now seek to explain how the substitution model is used to calculate the probability of a state $t$ evolving to a state $u$ along a given edge of a tree. Let us state upfront the parameters of the model:

\begin{align*}
    \lambda &= (\mu,\;\pi,\;\alpha,\;\beta,\;\phi)\\
    \mu &= \text{Substitution base rate}\\
    \pi &= \text{Stationary frequencies: } \pi_0, \pi_1 \in \left(0, 1\right) : \pi_0 + \pi_1 = 1.0\\
    \alpha &= \text{Shape for Among Site Rate Variation (ASRV)}\\
    \beta &= \text{Shape for Among Branch Rate Variation (ABRV)}\\
    \phi &= \text{ABRV Rate Assignments}
\end{align*}

The core of the substitution model is the rate matrix, which describes the rates at which states mutate into each other. These matrices can be quite complex, but we opt for the simplest choice, the Generalised Time Reversible (GTR) model.\footnote{Chang et al. calls this the Restriction Site Character (RSC) model; the two are equivalent in the case of binary traits.} We have already seen how the stationary frequencies $\pi_0$ and $\pi_1$ influence the likelihood calculation, but they are also at the heart of the binary GTR rate matrix. 
\begin{equation}
\textbf{Q} = \frac{1}{2\pi_0\pi_1} \begin{bmatrix}
-\pi_1 & \pi_1\\
\pi_0 & -\pi_0
\end{bmatrix} \text{ \ given that \ } \pi_0 + \pi_1 = 1.0
\end{equation}

As the length of time over which a trait evolves increases, the probability of it being in a state $u$ asymptopically approaches $\pi_u$, and the state it was initially in becomes irrelevant. To be more precise, for every branch $(a, b)$ in the tree $\psi$, for every trait $i$, the transition probabilities $Pr^i_{a,b}\left(u\,\vert\,t\right)$ are defined by the transition matrix $\textbf{P}^i_{a,b}$, where:

\begin{equation}
\textbf{P}^i_{a,b} = \exp\left(\textbf{Q} \cdot \delta_{a,b} \cdot \eta^i_{a,b}\right) \text{  \ where  \  } \eta^i_{a,b} = \mu \cdot \gamma_i \cdot \rho_{a,b}
\end{equation}

As previously stated, the value $\delta_{a,b}$ is the length (in years) of the branch $(a, b)$. The value $\eta^i_{a,b}$ is the rate for the trait $i$ on the branch $(a, b)$, calculated as the product of three rate parameters. The first rate,  $\mu$, is the base rate -- a global parameter that cancels out the units of the branch lengths and controls the overall rate of evolution. The second, $\gamma_i$, is the site rate,\footnote{The term `site' is a relic from bioinformatics, where traits typically correspond to specific sites on the genome} which is specific to this trait $i$. It is drawn from a Gamma distribution $\Gamma(\alpha, \frac{1}{\alpha})$. The third rate, $\rho_{a,b}$, is the branch rate, which is specific to this branch. It is drawn from a log-normal distribution $\log \mathcal{N}(-\frac{\beta^2}{2}, \beta^2)$. Both of these distributions have a mean of $1$, so regardless of their shapes the overall average rate is equal to $\mu$. The choices of distributions are partly informed by implementation pragamatics and partly by the flexibility in shape that can be attained by modfications to $\alpha$ and $\beta$. \footnote{This paragraph abstracts quite significantly over the specifics of how these rates are actually chosen, which shall be discussed later.}

At this point the reader may be questioning the necessity of allowing rates to vary both across sites and and across branches. However there is a strong justification for both laxities. Variation in site rate is necessary because words vary greatly in their volatility. Certain common words, in particular numerals, change incredibly rarely. Others are considerably more susceptible to replacement. Equally, branch rates must be variable because languages evolve at dramatically different rates. A failure to account for this fact was the downfall of much early research into quantitiative historical linguistics. The classic (though by no means sole) exemplar is the case of the Nordic languages; while Norwegians find Old Norse virtually incomprehensible, Icelanders can read it as easily as an Englishman can read Jane Austen. The reason for this is simply that the rate of change of Icelandic has been remarkably slow, while that of Norwegian has been reasonably fast.

\subsection{Prior Distributions}

The second part of equation \eqref{eqn:posterior} is the prior likelihood of the parameterisation. These terms express our baseline evolutionary model for phylogenies in general and language in particular.

The first prior term in \eqref{eqn:posterior} is $f(\psi\;|\;\omega)$, which expresses the prior likelihood of the tree $\psi$ conditioned on the set of parameters $\omega$. In turn, the parameters $\omega$ have their own prior distribution $f(\omega)$. Together, these terms fully implement the prior likelihood model for dated trees, informally known as the `tree prior'. The choice of tree prior is among the most impactful and contentious choices that a researcher needs to make when performing phylogenetic inference.

\subsubsection{Tree Constraints}

To be precise, the prior distribution over $\psi$ is actually composed of a general-purpose phylogenetic model and a set of dataset-specific constraints. We implement three such classes of constraints: tip calibrations, clade constraints, and ancestry constraints.

Tip calibrations constrain the ages of tips that are positioned at some depth in the past. They are implemented as windows of uniform prior density, with all times outside the window having zero prior likelihood. These calibrations are used to encode our convictions about the time periods in which ancient languages were used into the prior model, while giving the inference process freedom to infer the time-depth of these nodes if such a signal exists. As an example, in our Indo-European inferences, the well understood language of Old English has a relatively tight calibration window of (950, 1050) years before the present, while Tocharian B -- about which we know comparatively little -- has a wider window of (1200, 1500) years.

The second constraint class is clade constraints, which are improper priors that simply impose a very large likelihood penalty on topologies that do not contain a given clade. A clade, also known as a monophylectic group, is a set of taxa (i.e. languages) that are all more closely related to each other than they are to any other taxon. Intuitively, a clade is a group that can be pruned from the tree by snipping in exactly one place. Implementing such a constraint into a prior model is sensible when a clade has been established by scholars beyond any doubt.

The third constraint class is ancestory constraints, which are an extension of clade constraints that in addition to requiring that a specified set of taxa form a clade also nominate an additional taxon that is required to be their direct ancestor. Since taxa are exclusively assigned to tips it is not possible for a given taxon to be associated with an internal node of a phylogeny. However, it is possible to simulate ancestry by requiring the ancestral taxon to be joined to the internal node directly above its descendants' clade by a branch of negligable length. We achieve this by using extremely costly improper priors on the lengths of these branches.

We will return to the topic of constraints when we evaluate the suitability of different priors for phylolinguistic inference. For the time being, however, we set them aside. Just remember that whenever we say `tree prior', what we really mean is `prior model \textit{with constraints}'.

\subsubsection{Coalescent Priors}

In our research we focus mainly on a tree prior described by Drummond et al. in a 2005 paper. This paper provides a Bayesian methodology for sampling a tree model called the `generalized skyline plot'. For brevity we refer to this methodology as the `Bayesian skyline'. The theoretical basis for the generalised skyline plot is a stochastic process known as `coalescent theory', and for this reason the Drummond et al. tree prior, and related models, are sometimes known as `coalescent priors'.

We now present an overview of coalescent theory and its implementation as a tree prior in the form of the Bayesian skyline.

It is natural to consider evolutionary trees as being created by a `branching process'. However, the insight of coalescent theory is to consider a stochastic process in the opposite direction, starting with an initial set of lineages and moving backwards until they have all merged into a single lineage -- a `coalescent process'.

We will first consider how the coalescent process applies to just two lineages. They coalesce at a time depth drawn from an exponential distribution with mean $\theta$. In coalescent theory, this parameter is typically known as the `population', because in the biological context within which the theory was devised it is proportional to the size of the breeding population of the community of organisms in question. In the context of phylolinguistics it is perhaps best understood as simply being the inverse of the rate of coalescence.

When we have more than two lineages, each pair of lineages has a chance of coalescing according to this same distribution. Therefore, given $k$ lineages, the overall rate of coalescent events increases by a factor of $^{k}C_2$. Therefore the probability distribution for the time depth of the next coalescent event is the following:

\begin{equation}\label{eqn:coalescent}
Pr(t) = Exp\left(\frac{\binom{k}{2}}{\theta}\right) = \frac{k(k-1)}{2\theta} \cdot e^{-\frac{k(k-1)}{2\theta} t}
\end{equation}

Assume we have a dated phylogenetic tree $\psi$ with all $n$ tips positioned at time $t=0$. Then, moving backwards through time, we can divide its chronology into $n - 1$ intervals, with the first interval starting at $t=0$, and each interval ending at a coalescent event (of which there are necessarily $n-1$). Then every interval has some width $w_i$, which can be considered to be the time spent awaiting the coalescent event for that interval. Each interval also has a certain number of lineages, $k_i$ (which is clearly $n$ for the first interval, $n - 1$ for the second, and so on down to $2$ lineages for the final interval). We also associate some population parameter $\theta_i$ with each interval, which we collectively refer to as $\Theta$. Then it is reasonably intuitive to see that, following on from \eqref{eqn:coalescent}, the likelihood of $\psi$ is the following:

\begin{equation}\label{eqn:skyline}
Pr(\psi\;|\;\Theta) = \prod_{i = 1}^{n - 1}\; \frac{k_i(k_i-1)}{2\theta_i} \cdot e^{-\frac{k_i(k_i-1)}{2\theta_i} w_i}
\end{equation}

This is the `classic skyline plot'. There are only two added complexities that modify this model into the `generalised skyline plot' that is implemented in \textit{Dyr}. The first is that instead of selecting $n - 1$ values of $\theta$, we select $s$ values, where $s$ is a consideraly smaller number; typically $5$.\footnote{This value is pre-selected and not inferred by MCMC. The reader may fairly ask why; the reason is that changing $s$ changes the size of the parameter space. Stepping through variably-sized parameter spaces requires a considerably more complicated variant of MCMC called `reversible-jump'. This technique is not used by most phylolinguistic research and is consequently out-of-scope for our project.} We then group the $n - 1$ coalescent intervals into $s$ contiguous `generalised intervals' to which the $\theta$ values correspondingly apply. This is to prevent the population values being too heavily influenced by stochastical noise. The other necessary modfication is to allow for the number of lineages to change within a coalescent interval. I spare the reader the fiddly notation for these amendments since the underlying maths remains essentially the same.

The parameters of the generalised skyline plot that are inferred by the MCMC process are the sizes of the generalised intervals, $\Xi$, and the `population' parameters $\Theta$. Thus, for inferences using this tree prior, $\omega = (\Xi, \Theta)$. The parameters $\Xi$ have a uniform prior (with the proviso that all generalised intervals must contain at least one coalescent interval). However, $\Theta$ has the following scale-invariant prior, which applies population smoothing between adjacent generalised intervals.\footnote{This equation is slightly different to that given by Drummond et al. -- I believe theirs to be erroneous.}

\begin{equation}\label{eqn:thetaprior}
f(\Theta) = \frac{1}{\theta_1} \prod_{j=2}^s \theta_{j-1} \cdot e^{-\frac{\theta_j}{\theta_{j-1}}}
\end{equation}

For inferences using the generalised skyline plot, equation \eqref{eqn:thetaprior} effectively corresponds to $f(\omega)$ in \eqref{eqn:posterior}.

\subsubsection{Tree Prior Suitability}

We have chosen to implement the generalised skyline plot because it is the standard choice of tree prior for phylolinguistic inference. This is in spite of the fact that the theory underlying it does not have a convincing linguistic interpretation. Coalescent theory is predicated on a large population of individuals, inter-breeding at random, of which only a relatively tiny fraction are sampled. This makes sense in a bioinformatics context; most likely your dataset does not include an appreciable fraction of all the organisms in a species! However, in a linguistics context, this assumption does not hold up -- our broad dataset for Indo-European contains 94 languages, a non-trivial proportion of all Indo-European languages ever to exist (even with a liberal approach to the language-or-dialect question). It is also hard to reconcile our basic intuitions about language evolution with the stipulations of coalescent theory; in particular, that a population be haploid (languages don't always cleanly displace each other) and mate randomly (suggesting a reason why Tajik has had little influence on Icelandic is left as an exercise to the reader).

Rama is to my knowledge the only researcher to admit that the coalescent prior is hard to interpret in a linguistic context. He half-heartedly suggests that the observed languages are a sample of a larger haploid population of languages. This cannot, however, be reconciled with our knowledge that our Indo-European dataset comprises a reasonably large fraction of all the Indo-European languages spoken today (and, most likely, all those ever spoken). Rama also does not offer a suggestion to solve the random-breeding problem.

In my view, it is best to simply treat the troublesome `population' parameter as simply being an inferred `inverse rate of coalescence' and leave it at that. In this context, it is probably more sensible to use a constant-population coalescent prior (as Rama does) rather than the generalised skyline plot that is more generally preferred. Coalescent priors do nonetheless have some sensible properties, in particular that the rate of coalescence is proportional to the number of lineages. This means we can treat coalescence as a local phenomena, which is obviously desirable. The likelihood of a particular language diverging is very clearly not influenced by another language two continents away splitting in twain. This might sound too obvious to need stating, but it is a property not shared with the uniform prior, which Rama suggests as a viable alternative to the coalescent prior. This prior model assumes that coalescent events are evenly distributed between the root and the present day, which sounds sensible until one realises the implication that the likelihood of any given lineage diverging is reduced when the overall number of lineages increases.

An alternative to the dubiously justified coalescent prior and the demonstrably inappropriate uniform prior is the fossilised birth death prior. write some more here?

\subsection{Markov Chain Monte Carlo}

\section{Implementation}

\subsection{Program Structure}

\subsection{Elegance and Extensibility}

\section{Results}

\subsection{Comparison with Other Research}

\subsection{Performance Analysis}

\section{Evaluation and Future Steps}

\section{Conclusion}

\newpage

\bibliographystyle{ieeetran}
\bibliography{dyr}

\end{document}
